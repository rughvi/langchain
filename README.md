# ollama native on Mac M3

The following sateps explain how to run Ollama llm in Mac M3.

Steps:

1. Download ollama.dmg file
2. Install ollama.dmg file
3. Run the ollama after installation.
4. In cli, use the following command to fetch any of the ollama models.
   ollama pull gpt-oss:20b
5. Now we can run the code in main_1.ipynb

Note: 
1. Ollama by default runs in Apple's inbuilt GPU. No need to configure further.
2. Because of resource restriction Ollama llm takes time (1 minute roughly) to respond for the query.

